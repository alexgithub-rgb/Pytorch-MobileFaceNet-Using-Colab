{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16601,"status":"ok","timestamp":1655136093889,"user":{"displayName":"N26104214劉恩溢","userId":"03842490293544196844"},"user_tz":-480},"id":"bWSd_mEWAV86","outputId":"2064d45a-5720-4268-d430-a40554276285"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/My Drive/Colab Notebooks/face\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd gdrive/My Drive/Colab Notebooks/face"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ws896ggAkOb"},"outputs":[],"source":["# !git clone https://github.com/yeyupiaoling/Pytorch-MobileFaceNet.git"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17460,"status":"ok","timestamp":1655136111347,"user":{"displayName":"N26104214劉恩溢","userId":"03842490293544196844"},"user_tz":-480},"id":"IfDksrqhA2g1","outputId":"dff5d918-b482-4275-cf7b-c72747673edf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[K     |████████████████████████████████| 49.1 MB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n","Collecting graphviz<0.9.0,>=0.8.1\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.21.6)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2022.5.18.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.10.1\n","    Uninstalling graphviz-0.10.1:\n","      Successfully uninstalled graphviz-0.10.1\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting thop\n","  Downloading thop-0.1.0.post2206102148-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from thop) (1.11.0+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->thop) (4.2.0)\n","Installing collected packages: thop\n","Successfully installed thop-0.1.0.post2206102148\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting onnx\n","  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 22.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (4.2.0)\n","Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx) (1.15.0)\n","Installing collected packages: onnx\n","Successfully installed onnx-1.11.0\n"]}],"source":["!pip install mxnet\n","!pip install thop\n","!pip install onnx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZnFGphtAs1i"},"outputs":[],"source":["import os\n","os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/face/Pytorch-MobileFaceNet\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51266,"status":"ok","timestamp":1655128405308,"user":{"displayName":"N26104214劉恩溢","userId":"03842490293544196844"},"user_tz":-480},"id":"vc7bDdc4AyX9","outputId":"0220d6ec-8bbf-42ae-deaa-2683acb257c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["new_9\n","训练数据大小：7023，总类别为：5749\n","100% 7023/7023 [00:44<00:00, 157.08it/s]\n"]}],"source":["!python create_dataset.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":497370,"status":"ok","timestamp":1655137743412,"user":{"displayName":"N26104214劉恩溢","userId":"03842490293544196844"},"user_tz":-480},"id":"ORifQ7SxA0vA","outputId":"09837a11-4ccf-4107-9ca4-0cb48b9ec23a"},"outputs":[{"name":"stdout","output_type":"stream","text":["new8\n","-----------  Configuration Arguments -----------\n","batch_size: 20\n","gpus: 0\n","learning_rate: 0.001\n","num_epoch: 30\n","num_workers: 0\n","resume: None\n","save_model: save_model/new10\n","test_list_path: /content/gdrive/My Drive/Colab Notebooks/face/Pytorch-MobileFaceNet/dataset/lfw_test/lfw_test.txt\n","train_root_path: /content/gdrive/My Drive/Colab Notebooks/face/Pytorch-MobileFaceNet/dataset/train_data10\n","------------------------------------------------\n","正在加载数据标签...\n","数据加载完成，总数据量为：6102, 类别数量为：5749\n","[2022-06-13 16:20:48.289461] 总数据类别为：5749\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 56, 56]           1,728\n","       BatchNorm2d-2           [-1, 64, 56, 56]             128\n","             PReLU-3           [-1, 64, 56, 56]              64\n","         ConvBlock-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]             576\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","             PReLU-7           [-1, 64, 56, 56]              64\n","         ConvBlock-8           [-1, 64, 56, 56]               0\n","            Conv2d-9          [-1, 128, 56, 56]           8,192\n","      BatchNorm2d-10          [-1, 128, 56, 56]             256\n","            PReLU-11          [-1, 128, 56, 56]             128\n","        ConvBlock-12          [-1, 128, 56, 56]               0\n","           Conv2d-13          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-14          [-1, 128, 28, 28]             256\n","            PReLU-15          [-1, 128, 28, 28]             128\n","        ConvBlock-16          [-1, 128, 28, 28]               0\n","           Conv2d-17           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-18           [-1, 64, 28, 28]             128\n","      LinearBlock-19           [-1, 64, 28, 28]               0\n","        DepthWise-20           [-1, 64, 28, 28]               0\n","           Conv2d-21          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-22          [-1, 128, 28, 28]             256\n","            PReLU-23          [-1, 128, 28, 28]             128\n","        ConvBlock-24          [-1, 128, 28, 28]               0\n","           Conv2d-25          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-26          [-1, 128, 28, 28]             256\n","            PReLU-27          [-1, 128, 28, 28]             128\n","        ConvBlock-28          [-1, 128, 28, 28]               0\n","           Conv2d-29           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-30           [-1, 64, 28, 28]             128\n","      LinearBlock-31           [-1, 64, 28, 28]               0\n","DepthWiseResidual-32           [-1, 64, 28, 28]               0\n","           Conv2d-33          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-34          [-1, 128, 28, 28]             256\n","            PReLU-35          [-1, 128, 28, 28]             128\n","        ConvBlock-36          [-1, 128, 28, 28]               0\n","           Conv2d-37          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-38          [-1, 128, 28, 28]             256\n","            PReLU-39          [-1, 128, 28, 28]             128\n","        ConvBlock-40          [-1, 128, 28, 28]               0\n","           Conv2d-41           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-42           [-1, 64, 28, 28]             128\n","      LinearBlock-43           [-1, 64, 28, 28]               0\n","DepthWiseResidual-44           [-1, 64, 28, 28]               0\n","           Conv2d-45          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-46          [-1, 128, 28, 28]             256\n","            PReLU-47          [-1, 128, 28, 28]             128\n","        ConvBlock-48          [-1, 128, 28, 28]               0\n","           Conv2d-49          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","            PReLU-51          [-1, 128, 28, 28]             128\n","        ConvBlock-52          [-1, 128, 28, 28]               0\n","           Conv2d-53           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-54           [-1, 64, 28, 28]             128\n","      LinearBlock-55           [-1, 64, 28, 28]               0\n","DepthWiseResidual-56           [-1, 64, 28, 28]               0\n","           Conv2d-57          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-58          [-1, 128, 28, 28]             256\n","            PReLU-59          [-1, 128, 28, 28]             128\n","        ConvBlock-60          [-1, 128, 28, 28]               0\n","           Conv2d-61          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-62          [-1, 128, 28, 28]             256\n","            PReLU-63          [-1, 128, 28, 28]             128\n","        ConvBlock-64          [-1, 128, 28, 28]               0\n","           Conv2d-65           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-66           [-1, 64, 28, 28]             128\n","      LinearBlock-67           [-1, 64, 28, 28]               0\n","DepthWiseResidual-68           [-1, 64, 28, 28]               0\n","         Residual-69           [-1, 64, 28, 28]               0\n","           Conv2d-70          [-1, 256, 28, 28]          16,384\n","      BatchNorm2d-71          [-1, 256, 28, 28]             512\n","            PReLU-72          [-1, 256, 28, 28]             256\n","        ConvBlock-73          [-1, 256, 28, 28]               0\n","           Conv2d-74          [-1, 256, 14, 14]           2,304\n","      BatchNorm2d-75          [-1, 256, 14, 14]             512\n","            PReLU-76          [-1, 256, 14, 14]             256\n","        ConvBlock-77          [-1, 256, 14, 14]               0\n","           Conv2d-78           [-1, 64, 14, 14]          16,384\n","      BatchNorm2d-79           [-1, 64, 14, 14]             128\n","      LinearBlock-80           [-1, 64, 14, 14]               0\n","        DepthWise-81           [-1, 64, 14, 14]               0\n","           Conv2d-82          [-1, 256, 14, 14]          16,384\n","      BatchNorm2d-83          [-1, 256, 14, 14]             512\n","            PReLU-84          [-1, 256, 14, 14]             256\n","        ConvBlock-85          [-1, 256, 14, 14]               0\n","           Conv2d-86          [-1, 256, 14, 14]           2,304\n","      BatchNorm2d-87          [-1, 256, 14, 14]             512\n","            PReLU-88          [-1, 256, 14, 14]             256\n","        ConvBlock-89          [-1, 256, 14, 14]               0\n","           Conv2d-90           [-1, 64, 14, 14]          16,384\n","      BatchNorm2d-91           [-1, 64, 14, 14]             128\n","      LinearBlock-92           [-1, 64, 14, 14]               0\n","DepthWiseResidual-93           [-1, 64, 14, 14]               0\n","           Conv2d-94          [-1, 256, 14, 14]          16,384\n","      BatchNorm2d-95          [-1, 256, 14, 14]             512\n","            PReLU-96          [-1, 256, 14, 14]             256\n","        ConvBlock-97          [-1, 256, 14, 14]               0\n","           Conv2d-98          [-1, 256, 14, 14]           2,304\n","      BatchNorm2d-99          [-1, 256, 14, 14]             512\n","           PReLU-100          [-1, 256, 14, 14]             256\n","       ConvBlock-101          [-1, 256, 14, 14]               0\n","          Conv2d-102           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-103           [-1, 64, 14, 14]             128\n","     LinearBlock-104           [-1, 64, 14, 14]               0\n","DepthWiseResidual-105           [-1, 64, 14, 14]               0\n","          Conv2d-106          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-107          [-1, 256, 14, 14]             512\n","           PReLU-108          [-1, 256, 14, 14]             256\n","       ConvBlock-109          [-1, 256, 14, 14]               0\n","          Conv2d-110          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-111          [-1, 256, 14, 14]             512\n","           PReLU-112          [-1, 256, 14, 14]             256\n","       ConvBlock-113          [-1, 256, 14, 14]               0\n","          Conv2d-114           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-115           [-1, 64, 14, 14]             128\n","     LinearBlock-116           [-1, 64, 14, 14]               0\n","DepthWiseResidual-117           [-1, 64, 14, 14]               0\n","          Conv2d-118          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-119          [-1, 256, 14, 14]             512\n","           PReLU-120          [-1, 256, 14, 14]             256\n","       ConvBlock-121          [-1, 256, 14, 14]               0\n","          Conv2d-122          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-123          [-1, 256, 14, 14]             512\n","           PReLU-124          [-1, 256, 14, 14]             256\n","       ConvBlock-125          [-1, 256, 14, 14]               0\n","          Conv2d-126           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-127           [-1, 64, 14, 14]             128\n","     LinearBlock-128           [-1, 64, 14, 14]               0\n","DepthWiseResidual-129           [-1, 64, 14, 14]               0\n","          Conv2d-130          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-131          [-1, 256, 14, 14]             512\n","           PReLU-132          [-1, 256, 14, 14]             256\n","       ConvBlock-133          [-1, 256, 14, 14]               0\n","          Conv2d-134          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-135          [-1, 256, 14, 14]             512\n","           PReLU-136          [-1, 256, 14, 14]             256\n","       ConvBlock-137          [-1, 256, 14, 14]               0\n","          Conv2d-138           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-139           [-1, 64, 14, 14]             128\n","     LinearBlock-140           [-1, 64, 14, 14]               0\n","DepthWiseResidual-141           [-1, 64, 14, 14]               0\n","          Conv2d-142          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-143          [-1, 256, 14, 14]             512\n","           PReLU-144          [-1, 256, 14, 14]             256\n","       ConvBlock-145          [-1, 256, 14, 14]               0\n","          Conv2d-146          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-147          [-1, 256, 14, 14]             512\n","           PReLU-148          [-1, 256, 14, 14]             256\n","       ConvBlock-149          [-1, 256, 14, 14]               0\n","          Conv2d-150           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-151           [-1, 64, 14, 14]             128\n","     LinearBlock-152           [-1, 64, 14, 14]               0\n","DepthWiseResidual-153           [-1, 64, 14, 14]               0\n","        Residual-154           [-1, 64, 14, 14]               0\n","          Conv2d-155          [-1, 512, 14, 14]          32,768\n","     BatchNorm2d-156          [-1, 512, 14, 14]           1,024\n","           PReLU-157          [-1, 512, 14, 14]             512\n","       ConvBlock-158          [-1, 512, 14, 14]               0\n","          Conv2d-159            [-1, 512, 7, 7]           4,608\n","     BatchNorm2d-160            [-1, 512, 7, 7]           1,024\n","           PReLU-161            [-1, 512, 7, 7]             512\n","       ConvBlock-162            [-1, 512, 7, 7]               0\n","          Conv2d-163            [-1, 128, 7, 7]          65,536\n","     BatchNorm2d-164            [-1, 128, 7, 7]             256\n","     LinearBlock-165            [-1, 128, 7, 7]               0\n","       DepthWise-166            [-1, 128, 7, 7]               0\n","          Conv2d-167            [-1, 256, 7, 7]          32,768\n","     BatchNorm2d-168            [-1, 256, 7, 7]             512\n","           PReLU-169            [-1, 256, 7, 7]             256\n","       ConvBlock-170            [-1, 256, 7, 7]               0\n","          Conv2d-171            [-1, 256, 7, 7]           2,304\n","     BatchNorm2d-172            [-1, 256, 7, 7]             512\n","           PReLU-173            [-1, 256, 7, 7]             256\n","       ConvBlock-174            [-1, 256, 7, 7]               0\n","          Conv2d-175            [-1, 128, 7, 7]          32,768\n","     BatchNorm2d-176            [-1, 128, 7, 7]             256\n","     LinearBlock-177            [-1, 128, 7, 7]               0\n","DepthWiseResidual-178            [-1, 128, 7, 7]               0\n","          Conv2d-179            [-1, 256, 7, 7]          32,768\n","     BatchNorm2d-180            [-1, 256, 7, 7]             512\n","           PReLU-181            [-1, 256, 7, 7]             256\n","       ConvBlock-182            [-1, 256, 7, 7]               0\n","          Conv2d-183            [-1, 256, 7, 7]           2,304\n","     BatchNorm2d-184            [-1, 256, 7, 7]             512\n","           PReLU-185            [-1, 256, 7, 7]             256\n","       ConvBlock-186            [-1, 256, 7, 7]               0\n","          Conv2d-187            [-1, 128, 7, 7]          32,768\n","     BatchNorm2d-188            [-1, 128, 7, 7]             256\n","     LinearBlock-189            [-1, 128, 7, 7]               0\n","DepthWiseResidual-190            [-1, 128, 7, 7]               0\n","        Residual-191            [-1, 128, 7, 7]               0\n","          Conv2d-192            [-1, 512, 7, 7]          65,536\n","     BatchNorm2d-193            [-1, 512, 7, 7]           1,024\n","           PReLU-194            [-1, 512, 7, 7]             512\n","       ConvBlock-195            [-1, 512, 7, 7]               0\n","          Conv2d-196            [-1, 512, 1, 1]          25,088\n","     BatchNorm2d-197            [-1, 512, 1, 1]           1,024\n","     LinearBlock-198            [-1, 512, 1, 1]               0\n","         Flatten-199                  [-1, 512]               0\n","          Linear-200                  [-1, 512]         262,144\n","     BatchNorm1d-201                  [-1, 512]           1,024\n","================================================================\n","Total params: 953,856\n","Trainable params: 953,856\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.14\n","Forward/backward pass size (MB): 95.68\n","Params size (MB): 3.64\n","Estimated Total Size (MB): 99.46\n","----------------------------------------------------------------\n","/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:372: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  \"please use `get_last_lr()`.\", UserWarning)\n","[2022-06-13 16:20:58.072048] Train epoch 0, batch: 0/306, loss: 15.031298, accuracy: 0.000000, lr: 0.001000, eta: 15:08:31\n","[2022-06-13 16:21:18.095197] Train epoch 0, batch: 100/306, loss: 15.729231, accuracy: 0.000000, lr: 0.001000, eta: 0:10:20\n","[2022-06-13 16:21:23.993484] Train epoch 0, batch: 200/306, loss: 15.222089, accuracy: 0.000000, lr: 0.001000, eta: 0:32:57\n","[2022-06-13 16:21:28.733847] Train epoch 0, batch: 300/306, loss: 15.549006, accuracy: 0.000000, lr: 0.001000, eta: 0:09:33\n","======================================================================\n","======================================================================\n","[2022-06-13 16:21:29.916252] Train epoch 1, batch: 0/306, loss: 14.866666, accuracy: 0.000000, lr: 0.000640, eta: 0:09:33\n","[2022-06-13 16:21:34.650275] Train epoch 1, batch: 100/306, loss: 15.385931, accuracy: 0.000000, lr: 0.000640, eta: 0:09:25\n","[2022-06-13 16:21:39.502162] Train epoch 1, batch: 200/306, loss: 15.536572, accuracy: 0.000000, lr: 0.000640, eta: 0:09:52\n","[2022-06-13 16:21:44.209974] Train epoch 1, batch: 300/306, loss: 14.797285, accuracy: 0.000000, lr: 0.000640, eta: 0:09:17\n","======================================================================\n","======================================================================\n","[2022-06-13 16:21:45.235088] Train epoch 2, batch: 0/306, loss: 15.019445, accuracy: 0.000000, lr: 0.000512, eta: 0:08:52\n","[2022-06-13 16:21:50.143029] Train epoch 2, batch: 100/306, loss: 14.967117, accuracy: 0.000000, lr: 0.000512, eta: 0:09:21\n","[2022-06-13 16:21:55.038101] Train epoch 2, batch: 200/306, loss: 14.647089, accuracy: 0.000000, lr: 0.000512, eta: 0:09:28\n","[2022-06-13 16:21:59.866717] Train epoch 2, batch: 300/306, loss: 14.285045, accuracy: 0.000000, lr: 0.000512, eta: 0:09:25\n","======================================================================\n","======================================================================\n","[2022-06-13 16:22:00.910827] Train epoch 3, batch: 0/306, loss: 13.432101, accuracy: 0.000000, lr: 0.000410, eta: 0:08:40\n","[2022-06-13 16:22:05.796409] Train epoch 3, batch: 100/306, loss: 14.141208, accuracy: 0.000000, lr: 0.000410, eta: 0:09:41\n","[2022-06-13 16:22:10.733322] Train epoch 3, batch: 200/306, loss: 13.873380, accuracy: 0.000000, lr: 0.000410, eta: 0:09:06\n","[2022-06-13 16:22:15.659785] Train epoch 3, batch: 300/306, loss: 14.214861, accuracy: 0.000000, lr: 0.000410, eta: 0:09:14\n","======================================================================\n","======================================================================\n","[2022-06-13 16:22:16.707275] Train epoch 4, batch: 0/306, loss: 13.610014, accuracy: 0.000000, lr: 0.000328, eta: 0:08:11\n","[2022-06-13 16:22:21.726201] Train epoch 4, batch: 100/306, loss: 12.917971, accuracy: 0.000000, lr: 0.000328, eta: 0:09:33\n","[2022-06-13 16:22:26.669904] Train epoch 4, batch: 200/306, loss: 12.878527, accuracy: 0.000000, lr: 0.000328, eta: 0:08:53\n","[2022-06-13 16:22:31.531833] Train epoch 4, batch: 300/306, loss: 12.605321, accuracy: 0.000000, lr: 0.000328, eta: 0:08:11\n","======================================================================\n","======================================================================\n","[2022-06-13 16:22:32.596919] Train epoch 5, batch: 0/306, loss: 12.651330, accuracy: 0.000000, lr: 0.000262, eta: 0:07:44\n","[2022-06-13 16:22:37.553077] Train epoch 5, batch: 100/306, loss: 12.663505, accuracy: 0.000000, lr: 0.000262, eta: 0:08:13\n","[2022-06-13 16:22:42.438190] Train epoch 5, batch: 200/306, loss: 12.125456, accuracy: 0.000000, lr: 0.000262, eta: 0:08:15\n","[2022-06-13 16:22:47.273444] Train epoch 5, batch: 300/306, loss: 12.488356, accuracy: 0.000000, lr: 0.000262, eta: 0:08:11\n","======================================================================\n","======================================================================\n","[2022-06-13 16:22:48.314726] Train epoch 6, batch: 0/306, loss: 11.709986, accuracy: 0.000000, lr: 0.000210, eta: 0:07:35\n","[2022-06-13 16:22:53.166072] Train epoch 6, batch: 100/306, loss: 12.760710, accuracy: 0.000000, lr: 0.000210, eta: 0:07:40\n","[2022-06-13 16:22:57.985018] Train epoch 6, batch: 200/306, loss: 13.147855, accuracy: 0.000000, lr: 0.000210, eta: 0:07:19\n","[2022-06-13 16:23:02.777478] Train epoch 6, batch: 300/306, loss: 12.814100, accuracy: 0.000000, lr: 0.000210, eta: 0:07:59\n","======================================================================\n","======================================================================\n","[2022-06-13 16:23:03.825612] Train epoch 7, batch: 0/306, loss: 12.888613, accuracy: 0.000000, lr: 0.000168, eta: 0:07:21\n","[2022-06-13 16:23:08.684469] Train epoch 7, batch: 100/306, loss: 14.052111, accuracy: 0.000000, lr: 0.000168, eta: 0:08:35\n","[2022-06-13 16:23:13.502685] Train epoch 7, batch: 200/306, loss: 13.361483, accuracy: 0.000000, lr: 0.000168, eta: 0:07:30\n","[2022-06-13 16:23:18.292237] Train epoch 7, batch: 300/306, loss: 13.342280, accuracy: 0.000000, lr: 0.000168, eta: 0:07:41\n","======================================================================\n","======================================================================\n","[2022-06-13 16:23:19.354996] Train epoch 8, batch: 0/306, loss: 13.524381, accuracy: 0.000000, lr: 0.000134, eta: 0:08:02\n","[2022-06-13 16:23:24.169093] Train epoch 8, batch: 100/306, loss: 13.682388, accuracy: 0.000000, lr: 0.000134, eta: 0:07:27\n","[2022-06-13 16:23:29.030841] Train epoch 8, batch: 200/306, loss: 13.079971, accuracy: 0.000000, lr: 0.000134, eta: 0:07:13\n","[2022-06-13 16:23:33.847918] Train epoch 8, batch: 300/306, loss: 13.290072, accuracy: 0.000000, lr: 0.000134, eta: 0:07:08\n","======================================================================\n","======================================================================\n","[2022-06-13 16:23:34.892219] Train epoch 9, batch: 0/306, loss: 13.013311, accuracy: 0.000000, lr: 0.000107, eta: 0:06:41\n","[2022-06-13 16:23:39.797340] Train epoch 9, batch: 100/306, loss: 12.765584, accuracy: 0.000000, lr: 0.000107, eta: 0:07:00\n","[2022-06-13 16:23:44.663779] Train epoch 9, batch: 200/306, loss: 12.949623, accuracy: 0.000000, lr: 0.000107, eta: 0:07:24\n","[2022-06-13 16:23:49.510120] Train epoch 9, batch: 300/306, loss: 12.944583, accuracy: 0.000000, lr: 0.000107, eta: 0:06:59\n","======================================================================\n","======================================================================\n","[2022-06-13 16:23:50.534368] Train epoch 10, batch: 0/306, loss: 12.376616, accuracy: 0.000000, lr: 0.000086, eta: 0:06:17\n","[2022-06-13 16:23:55.370464] Train epoch 10, batch: 100/306, loss: 13.035360, accuracy: 0.000000, lr: 0.000086, eta: 0:06:53\n","[2022-06-13 16:24:00.272898] Train epoch 10, batch: 200/306, loss: 13.239848, accuracy: 0.000000, lr: 0.000086, eta: 0:06:45\n","[2022-06-13 16:24:05.095446] Train epoch 10, batch: 300/306, loss: 12.940681, accuracy: 0.000000, lr: 0.000086, eta: 0:06:29\n","======================================================================\n","======================================================================\n","[2022-06-13 16:24:06.139468] Train epoch 11, batch: 0/306, loss: 12.633823, accuracy: 0.000000, lr: 0.000069, eta: 0:05:54\n","[2022-06-13 16:24:11.007116] Train epoch 11, batch: 100/306, loss: 12.429955, accuracy: 0.000000, lr: 0.000069, eta: 0:07:01\n","[2022-06-13 16:24:15.847998] Train epoch 11, batch: 200/306, loss: 12.323559, accuracy: 0.000000, lr: 0.000069, eta: 0:06:23\n","[2022-06-13 16:24:20.626359] Train epoch 11, batch: 300/306, loss: 12.179144, accuracy: 0.000000, lr: 0.000069, eta: 0:06:13\n","======================================================================\n","======================================================================\n","[2022-06-13 16:24:21.715047] Train epoch 12, batch: 0/306, loss: 12.118423, accuracy: 0.000000, lr: 0.000055, eta: 0:05:46\n","[2022-06-13 16:24:27.209886] Train epoch 12, batch: 100/306, loss: 12.127904, accuracy: 0.000000, lr: 0.000055, eta: 0:06:03\n","[2022-06-13 16:24:32.093905] Train epoch 12, batch: 200/306, loss: 12.446525, accuracy: 0.000000, lr: 0.000055, eta: 0:05:51\n","[2022-06-13 16:24:36.887401] Train epoch 12, batch: 300/306, loss: 12.475111, accuracy: 0.000000, lr: 0.000055, eta: 0:05:49\n","======================================================================\n","======================================================================\n","[2022-06-13 16:24:37.937037] Train epoch 13, batch: 0/306, loss: 12.046991, accuracy: 0.000000, lr: 0.000044, eta: 0:05:27\n","[2022-06-13 16:24:42.782893] Train epoch 13, batch: 100/306, loss: 12.297235, accuracy: 0.000000, lr: 0.000044, eta: 0:06:35\n","[2022-06-13 16:24:47.612747] Train epoch 13, batch: 200/306, loss: 11.974768, accuracy: 0.000000, lr: 0.000044, eta: 0:05:27\n","[2022-06-13 16:24:52.464483] Train epoch 13, batch: 300/306, loss: 12.044176, accuracy: 0.000000, lr: 0.000044, eta: 0:05:39\n","======================================================================\n","======================================================================\n","[2022-06-13 16:24:53.496320] Train epoch 14, batch: 0/306, loss: 11.964540, accuracy: 0.000000, lr: 0.000035, eta: 0:04:58\n","[2022-06-13 16:24:58.355861] Train epoch 14, batch: 100/306, loss: 12.155218, accuracy: 0.000000, lr: 0.000035, eta: 0:06:41\n","[2022-06-13 16:25:03.216366] Train epoch 14, batch: 200/306, loss: 12.097478, accuracy: 0.000000, lr: 0.000035, eta: 0:05:23\n","[2022-06-13 16:25:08.021183] Train epoch 14, batch: 300/306, loss: 12.171988, accuracy: 0.000000, lr: 0.000035, eta: 0:05:09\n","======================================================================\n","======================================================================\n","[2022-06-13 16:25:09.040122] Train epoch 15, batch: 0/306, loss: 12.107195, accuracy: 0.000000, lr: 0.000028, eta: 0:04:35\n","[2022-06-13 16:25:13.902492] Train epoch 15, batch: 100/306, loss: 11.858212, accuracy: 0.000000, lr: 0.000028, eta: 0:05:28\n","[2022-06-13 16:25:18.735833] Train epoch 15, batch: 200/306, loss: 12.584094, accuracy: 0.000000, lr: 0.000028, eta: 0:04:55\n","[2022-06-13 16:25:23.549739] Train epoch 15, batch: 300/306, loss: 11.962803, accuracy: 0.000000, lr: 0.000028, eta: 0:04:46\n","======================================================================\n","======================================================================\n","[2022-06-13 16:25:24.594456] Train epoch 16, batch: 0/306, loss: 11.899192, accuracy: 0.000000, lr: 0.000023, eta: 0:04:33\n","[2022-06-13 16:25:29.415012] Train epoch 16, batch: 100/306, loss: 12.193388, accuracy: 0.000000, lr: 0.000023, eta: 0:04:49\n","[2022-06-13 16:25:34.330160] Train epoch 16, batch: 200/306, loss: 12.078370, accuracy: 0.000000, lr: 0.000023, eta: 0:04:27\n","[2022-06-13 16:25:39.142901] Train epoch 16, batch: 300/306, loss: 11.946641, accuracy: 0.000000, lr: 0.000023, eta: 0:04:23\n","======================================================================\n","======================================================================\n","[2022-06-13 16:25:40.185217] Train epoch 17, batch: 0/306, loss: 11.917150, accuracy: 0.000000, lr: 0.000018, eta: 0:04:02\n","[2022-06-13 16:25:45.080327] Train epoch 17, batch: 100/306, loss: 12.672476, accuracy: 0.000000, lr: 0.000018, eta: 0:05:11\n","[2022-06-13 16:25:49.929352] Train epoch 17, batch: 200/306, loss: 11.812865, accuracy: 0.000000, lr: 0.000018, eta: 0:04:11\n","[2022-06-13 16:25:54.742470] Train epoch 17, batch: 300/306, loss: 12.301613, accuracy: 0.000000, lr: 0.000018, eta: 0:04:05\n","======================================================================\n","======================================================================\n","[2022-06-13 16:25:55.779949] Train epoch 18, batch: 0/306, loss: 11.970637, accuracy: 0.000000, lr: 0.000014, eta: 0:03:40\n","[2022-06-13 16:26:00.641891] Train epoch 18, batch: 100/306, loss: 11.769399, accuracy: 0.000000, lr: 0.000014, eta: 0:04:05\n","[2022-06-13 16:26:05.459258] Train epoch 18, batch: 200/306, loss: 11.778181, accuracy: 0.000000, lr: 0.000014, eta: 0:03:36\n","[2022-06-13 16:26:10.257153] Train epoch 18, batch: 300/306, loss: 11.355740, accuracy: 0.000000, lr: 0.000014, eta: 0:03:46\n","======================================================================\n","======================================================================\n","[2022-06-13 16:26:11.294914] Train epoch 19, batch: 0/306, loss: 11.240712, accuracy: 0.000000, lr: 0.000012, eta: 0:03:29\n","[2022-06-13 16:26:16.161664] Train epoch 19, batch: 100/306, loss: 11.500410, accuracy: 0.000000, lr: 0.000012, eta: 0:03:40\n","[2022-06-13 16:26:21.011807] Train epoch 19, batch: 200/306, loss: 12.089852, accuracy: 0.000000, lr: 0.000012, eta: 0:03:33\n","[2022-06-13 16:26:25.840738] Train epoch 19, batch: 300/306, loss: 11.571867, accuracy: 0.000000, lr: 0.000012, eta: 0:03:24\n","======================================================================\n","======================================================================\n","[2022-06-13 16:26:26.860859] Train epoch 20, batch: 0/306, loss: 11.153112, accuracy: 0.000000, lr: 0.000009, eta: 0:03:06\n","[2022-06-13 16:26:31.676749] Train epoch 20, batch: 100/306, loss: 11.425902, accuracy: 0.000000, lr: 0.000009, eta: 0:03:25\n","[2022-06-13 16:26:36.538245] Train epoch 20, batch: 200/306, loss: 11.791456, accuracy: 0.000000, lr: 0.000009, eta: 0:03:12\n","[2022-06-13 16:26:41.350687] Train epoch 20, batch: 300/306, loss: 11.561472, accuracy: 0.000000, lr: 0.000009, eta: 0:03:10\n","======================================================================\n","======================================================================\n","[2022-06-13 16:26:42.374019] Train epoch 21, batch: 0/306, loss: 11.797812, accuracy: 0.000000, lr: 0.000007, eta: 0:02:52\n","[2022-06-13 16:26:47.211636] Train epoch 21, batch: 100/306, loss: 12.012072, accuracy: 0.000000, lr: 0.000007, eta: 0:03:17\n","[2022-06-13 16:26:52.078821] Train epoch 21, batch: 200/306, loss: 11.748045, accuracy: 0.000000, lr: 0.000007, eta: 0:02:55\n","[2022-06-13 16:26:56.875241] Train epoch 21, batch: 300/306, loss: 11.819687, accuracy: 0.000000, lr: 0.000007, eta: 0:02:46\n","======================================================================\n","======================================================================\n","[2022-06-13 16:26:57.923621] Train epoch 22, batch: 0/306, loss: 11.666845, accuracy: 0.000000, lr: 0.000006, eta: 0:02:31\n","[2022-06-13 16:27:02.765227] Train epoch 22, batch: 100/306, loss: 11.455092, accuracy: 0.000000, lr: 0.000006, eta: 0:02:54\n","[2022-06-13 16:27:07.606253] Train epoch 22, batch: 200/306, loss: 11.527376, accuracy: 0.000000, lr: 0.000006, eta: 0:02:29\n","[2022-06-13 16:27:12.406467] Train epoch 22, batch: 300/306, loss: 12.035650, accuracy: 0.000000, lr: 0.000006, eta: 0:02:22\n","======================================================================\n","======================================================================\n","[2022-06-13 16:27:13.430023] Train epoch 23, batch: 0/306, loss: 11.632789, accuracy: 0.000000, lr: 0.000005, eta: 0:02:09\n","[2022-06-13 16:27:18.275583] Train epoch 23, batch: 100/306, loss: 11.824572, accuracy: 0.000000, lr: 0.000005, eta: 0:02:18\n","[2022-06-13 16:27:23.179169] Train epoch 23, batch: 200/306, loss: 11.447027, accuracy: 0.000000, lr: 0.000005, eta: 0:02:09\n","[2022-06-13 16:27:27.977496] Train epoch 23, batch: 300/306, loss: 11.798704, accuracy: 0.000000, lr: 0.000005, eta: 0:02:03\n","======================================================================\n","======================================================================\n","[2022-06-13 16:27:29.013862] Train epoch 24, batch: 0/306, loss: 11.642660, accuracy: 0.000000, lr: 0.000004, eta: 0:01:52\n","[2022-06-13 16:27:33.843690] Train epoch 24, batch: 100/306, loss: 11.652662, accuracy: 0.000000, lr: 0.000004, eta: 0:01:59\n","[2022-06-13 16:27:38.711364] Train epoch 24, batch: 200/306, loss: 11.928222, accuracy: 0.000000, lr: 0.000004, eta: 0:01:52\n","[2022-06-13 16:27:43.522517] Train epoch 24, batch: 300/306, loss: 11.471614, accuracy: 0.000000, lr: 0.000004, eta: 0:01:44\n","======================================================================\n","======================================================================\n","[2022-06-13 16:27:44.574458] Train epoch 25, batch: 0/306, loss: 11.351824, accuracy: 0.000000, lr: 0.000003, eta: 0:01:32\n","[2022-06-13 16:27:49.429948] Train epoch 25, batch: 100/306, loss: 11.448308, accuracy: 0.000000, lr: 0.000003, eta: 0:01:42\n","[2022-06-13 16:27:54.283206] Train epoch 25, batch: 200/306, loss: 11.563479, accuracy: 0.000000, lr: 0.000003, eta: 0:01:30\n","[2022-06-13 16:27:59.098309] Train epoch 25, batch: 300/306, loss: 11.895737, accuracy: 0.000000, lr: 0.000003, eta: 0:01:21\n","======================================================================\n","======================================================================\n","[2022-06-13 16:28:00.148823] Train epoch 26, batch: 0/306, loss: 11.190510, accuracy: 0.000000, lr: 0.000002, eta: 0:01:14\n","[2022-06-13 16:28:05.038548] Train epoch 26, batch: 100/306, loss: 12.043610, accuracy: 0.000000, lr: 0.000002, eta: 0:01:17\n","[2022-06-13 16:28:09.905548] Train epoch 26, batch: 200/306, loss: 11.526279, accuracy: 0.000000, lr: 0.000002, eta: 0:01:09\n","[2022-06-13 16:28:14.743809] Train epoch 26, batch: 300/306, loss: 11.290228, accuracy: 0.000000, lr: 0.000002, eta: 0:01:04\n","======================================================================\n","======================================================================\n","[2022-06-13 16:28:15.758532] Train epoch 27, batch: 0/306, loss: 11.617467, accuracy: 0.000000, lr: 0.000002, eta: 0:00:59\n","[2022-06-13 16:28:20.605821] Train epoch 27, batch: 100/306, loss: 11.832485, accuracy: 0.000000, lr: 0.000002, eta: 0:00:54\n","[2022-06-13 16:28:25.518583] Train epoch 27, batch: 200/306, loss: 11.340749, accuracy: 0.000000, lr: 0.000002, eta: 0:00:48\n","[2022-06-13 16:28:30.346260] Train epoch 27, batch: 300/306, loss: 11.051503, accuracy: 0.000000, lr: 0.000002, eta: 0:00:41\n","======================================================================\n","======================================================================\n","[2022-06-13 16:28:31.384688] Train epoch 28, batch: 0/306, loss: 11.895064, accuracy: 0.000000, lr: 0.000002, eta: 0:00:38\n","[2022-06-13 16:28:36.255507] Train epoch 28, batch: 100/306, loss: 11.616693, accuracy: 0.000000, lr: 0.000002, eta: 0:00:33\n","[2022-06-13 16:28:41.092398] Train epoch 28, batch: 200/306, loss: 11.575956, accuracy: 0.000000, lr: 0.000002, eta: 0:00:27\n","[2022-06-13 16:28:45.903449] Train epoch 28, batch: 300/306, loss: 11.645682, accuracy: 0.000000, lr: 0.000002, eta: 0:00:20\n","======================================================================\n","======================================================================\n","[2022-06-13 16:28:46.937230] Train epoch 29, batch: 0/306, loss: 11.725615, accuracy: 0.000000, lr: 0.000001, eta: 0:00:18\n","[2022-06-13 16:28:51.823643] Train epoch 29, batch: 100/306, loss: 11.535716, accuracy: 0.000000, lr: 0.000001, eta: 0:00:12\n","[2022-06-13 16:28:56.703234] Train epoch 29, batch: 200/306, loss: 11.917761, accuracy: 0.000000, lr: 0.000001, eta: 0:00:07\n","[2022-06-13 16:29:01.522352] Train epoch 29, batch: 300/306, loss: 11.387517, accuracy: 0.000000, lr: 0.000001, eta: 0:00:00\n","======================================================================\n","======================================================================\n"]}],"source":["! python train.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12091,"status":"ok","timestamp":1655138184577,"user":{"displayName":"N26104214劉恩溢","userId":"03842490293544196844"},"user_tz":-480},"id":"Yg5y-A0rFBJJ","outputId":"b1007b83-81a1-4c2c-8467-c2bb3a3eb210"},"outputs":[{"name":"stdout","output_type":"stream","text":["new\n","-----------  Configuration Arguments -----------\n","batch_size: 64\n","model_path: save_model/new10/mobilefacenet.pth\n","test_list_path: dataset/lfw_test/lfw_test.txt\n","------------------------------------------------\n","/usr/local/lib/python3.7/dist-packages/torch/serialization.py:709: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n","  \" silence this warning)\", UserWarning)\n","100% 1104/1104 [00:06<00:00, 182.02it/s]\n","准确率为：0.616147, 最优阈值为：0.646123\n"]}],"source":["! python eval.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2036,"status":"ok","timestamp":1655135323583,"user":{"displayName":"N26104214劉恩溢","userId":"03842490293544196844"},"user_tz":-480},"id":"J5jbkZcRMM-A","outputId":"1a0658ed-0741-443a-f233-a9083eea2115"},"outputs":[{"name":"stdout","output_type":"stream","text":["new1\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 56, 56]           1,728\n","       BatchNorm2d-2           [-1, 64, 56, 56]             128\n","             PReLU-3           [-1, 64, 56, 56]              64\n","         ConvBlock-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]             576\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","             PReLU-7           [-1, 64, 56, 56]              64\n","         ConvBlock-8           [-1, 64, 56, 56]               0\n","            Conv2d-9          [-1, 128, 56, 56]           8,192\n","      BatchNorm2d-10          [-1, 128, 56, 56]             256\n","            PReLU-11          [-1, 128, 56, 56]             128\n","        ConvBlock-12          [-1, 128, 56, 56]               0\n","           Conv2d-13          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-14          [-1, 128, 28, 28]             256\n","            PReLU-15          [-1, 128, 28, 28]             128\n","        ConvBlock-16          [-1, 128, 28, 28]               0\n","           Conv2d-17           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-18           [-1, 64, 28, 28]             128\n","      LinearBlock-19           [-1, 64, 28, 28]               0\n","        DepthWise-20           [-1, 64, 28, 28]               0\n","           Conv2d-21          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-22          [-1, 128, 28, 28]             256\n","            PReLU-23          [-1, 128, 28, 28]             128\n","        ConvBlock-24          [-1, 128, 28, 28]               0\n","           Conv2d-25          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-26          [-1, 128, 28, 28]             256\n","            PReLU-27          [-1, 128, 28, 28]             128\n","        ConvBlock-28          [-1, 128, 28, 28]               0\n","           Conv2d-29           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-30           [-1, 64, 28, 28]             128\n","      LinearBlock-31           [-1, 64, 28, 28]               0\n","DepthWiseResidual-32           [-1, 64, 28, 28]               0\n","           Conv2d-33          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-34          [-1, 128, 28, 28]             256\n","            PReLU-35          [-1, 128, 28, 28]             128\n","        ConvBlock-36          [-1, 128, 28, 28]               0\n","           Conv2d-37          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-38          [-1, 128, 28, 28]             256\n","            PReLU-39          [-1, 128, 28, 28]             128\n","        ConvBlock-40          [-1, 128, 28, 28]               0\n","           Conv2d-41           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-42           [-1, 64, 28, 28]             128\n","      LinearBlock-43           [-1, 64, 28, 28]               0\n","DepthWiseResidual-44           [-1, 64, 28, 28]               0\n","           Conv2d-45          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-46          [-1, 128, 28, 28]             256\n","            PReLU-47          [-1, 128, 28, 28]             128\n","        ConvBlock-48          [-1, 128, 28, 28]               0\n","           Conv2d-49          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","            PReLU-51          [-1, 128, 28, 28]             128\n","        ConvBlock-52          [-1, 128, 28, 28]               0\n","           Conv2d-53           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-54           [-1, 64, 28, 28]             128\n","      LinearBlock-55           [-1, 64, 28, 28]               0\n","DepthWiseResidual-56           [-1, 64, 28, 28]               0\n","           Conv2d-57          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-58          [-1, 128, 28, 28]             256\n","            PReLU-59          [-1, 128, 28, 28]             128\n","        ConvBlock-60          [-1, 128, 28, 28]               0\n","           Conv2d-61          [-1, 128, 28, 28]           1,152\n","      BatchNorm2d-62          [-1, 128, 28, 28]             256\n","            PReLU-63          [-1, 128, 28, 28]             128\n","        ConvBlock-64          [-1, 128, 28, 28]               0\n","           Conv2d-65           [-1, 64, 28, 28]           8,192\n","      BatchNorm2d-66           [-1, 64, 28, 28]             128\n","      LinearBlock-67           [-1, 64, 28, 28]               0\n","DepthWiseResidual-68           [-1, 64, 28, 28]               0\n","         Residual-69           [-1, 64, 28, 28]               0\n","           Conv2d-70          [-1, 256, 28, 28]          16,384\n","      BatchNorm2d-71          [-1, 256, 28, 28]             512\n","            PReLU-72          [-1, 256, 28, 28]             256\n","        ConvBlock-73          [-1, 256, 28, 28]               0\n","           Conv2d-74          [-1, 256, 14, 14]           2,304\n","      BatchNorm2d-75          [-1, 256, 14, 14]             512\n","            PReLU-76          [-1, 256, 14, 14]             256\n","        ConvBlock-77          [-1, 256, 14, 14]               0\n","           Conv2d-78           [-1, 64, 14, 14]          16,384\n","      BatchNorm2d-79           [-1, 64, 14, 14]             128\n","      LinearBlock-80           [-1, 64, 14, 14]               0\n","        DepthWise-81           [-1, 64, 14, 14]               0\n","           Conv2d-82          [-1, 256, 14, 14]          16,384\n","      BatchNorm2d-83          [-1, 256, 14, 14]             512\n","            PReLU-84          [-1, 256, 14, 14]             256\n","        ConvBlock-85          [-1, 256, 14, 14]               0\n","           Conv2d-86          [-1, 256, 14, 14]           2,304\n","      BatchNorm2d-87          [-1, 256, 14, 14]             512\n","            PReLU-88          [-1, 256, 14, 14]             256\n","        ConvBlock-89          [-1, 256, 14, 14]               0\n","           Conv2d-90           [-1, 64, 14, 14]          16,384\n","      BatchNorm2d-91           [-1, 64, 14, 14]             128\n","      LinearBlock-92           [-1, 64, 14, 14]               0\n","DepthWiseResidual-93           [-1, 64, 14, 14]               0\n","           Conv2d-94          [-1, 256, 14, 14]          16,384\n","      BatchNorm2d-95          [-1, 256, 14, 14]             512\n","            PReLU-96          [-1, 256, 14, 14]             256\n","        ConvBlock-97          [-1, 256, 14, 14]               0\n","           Conv2d-98          [-1, 256, 14, 14]           2,304\n","      BatchNorm2d-99          [-1, 256, 14, 14]             512\n","           PReLU-100          [-1, 256, 14, 14]             256\n","       ConvBlock-101          [-1, 256, 14, 14]               0\n","          Conv2d-102           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-103           [-1, 64, 14, 14]             128\n","     LinearBlock-104           [-1, 64, 14, 14]               0\n","DepthWiseResidual-105           [-1, 64, 14, 14]               0\n","          Conv2d-106          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-107          [-1, 256, 14, 14]             512\n","           PReLU-108          [-1, 256, 14, 14]             256\n","       ConvBlock-109          [-1, 256, 14, 14]               0\n","          Conv2d-110          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-111          [-1, 256, 14, 14]             512\n","           PReLU-112          [-1, 256, 14, 14]             256\n","       ConvBlock-113          [-1, 256, 14, 14]               0\n","          Conv2d-114           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-115           [-1, 64, 14, 14]             128\n","     LinearBlock-116           [-1, 64, 14, 14]               0\n","DepthWiseResidual-117           [-1, 64, 14, 14]               0\n","          Conv2d-118          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-119          [-1, 256, 14, 14]             512\n","           PReLU-120          [-1, 256, 14, 14]             256\n","       ConvBlock-121          [-1, 256, 14, 14]               0\n","          Conv2d-122          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-123          [-1, 256, 14, 14]             512\n","           PReLU-124          [-1, 256, 14, 14]             256\n","       ConvBlock-125          [-1, 256, 14, 14]               0\n","          Conv2d-126           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-127           [-1, 64, 14, 14]             128\n","     LinearBlock-128           [-1, 64, 14, 14]               0\n","DepthWiseResidual-129           [-1, 64, 14, 14]               0\n","          Conv2d-130          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-131          [-1, 256, 14, 14]             512\n","           PReLU-132          [-1, 256, 14, 14]             256\n","       ConvBlock-133          [-1, 256, 14, 14]               0\n","          Conv2d-134          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-135          [-1, 256, 14, 14]             512\n","           PReLU-136          [-1, 256, 14, 14]             256\n","       ConvBlock-137          [-1, 256, 14, 14]               0\n","          Conv2d-138           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-139           [-1, 64, 14, 14]             128\n","     LinearBlock-140           [-1, 64, 14, 14]               0\n","DepthWiseResidual-141           [-1, 64, 14, 14]               0\n","          Conv2d-142          [-1, 256, 14, 14]          16,384\n","     BatchNorm2d-143          [-1, 256, 14, 14]             512\n","           PReLU-144          [-1, 256, 14, 14]             256\n","       ConvBlock-145          [-1, 256, 14, 14]               0\n","          Conv2d-146          [-1, 256, 14, 14]           2,304\n","     BatchNorm2d-147          [-1, 256, 14, 14]             512\n","           PReLU-148          [-1, 256, 14, 14]             256\n","       ConvBlock-149          [-1, 256, 14, 14]               0\n","          Conv2d-150           [-1, 64, 14, 14]          16,384\n","     BatchNorm2d-151           [-1, 64, 14, 14]             128\n","     LinearBlock-152           [-1, 64, 14, 14]               0\n","DepthWiseResidual-153           [-1, 64, 14, 14]               0\n","        Residual-154           [-1, 64, 14, 14]               0\n","          Conv2d-155          [-1, 512, 14, 14]          32,768\n","     BatchNorm2d-156          [-1, 512, 14, 14]           1,024\n","           PReLU-157          [-1, 512, 14, 14]             512\n","       ConvBlock-158          [-1, 512, 14, 14]               0\n","          Conv2d-159            [-1, 512, 7, 7]           4,608\n","     BatchNorm2d-160            [-1, 512, 7, 7]           1,024\n","           PReLU-161            [-1, 512, 7, 7]             512\n","       ConvBlock-162            [-1, 512, 7, 7]               0\n","          Conv2d-163            [-1, 128, 7, 7]          65,536\n","     BatchNorm2d-164            [-1, 128, 7, 7]             256\n","     LinearBlock-165            [-1, 128, 7, 7]               0\n","       DepthWise-166            [-1, 128, 7, 7]               0\n","          Conv2d-167            [-1, 256, 7, 7]          32,768\n","     BatchNorm2d-168            [-1, 256, 7, 7]             512\n","           PReLU-169            [-1, 256, 7, 7]             256\n","       ConvBlock-170            [-1, 256, 7, 7]               0\n","          Conv2d-171            [-1, 256, 7, 7]           2,304\n","     BatchNorm2d-172            [-1, 256, 7, 7]             512\n","           PReLU-173            [-1, 256, 7, 7]             256\n","       ConvBlock-174            [-1, 256, 7, 7]               0\n","          Conv2d-175            [-1, 128, 7, 7]          32,768\n","     BatchNorm2d-176            [-1, 128, 7, 7]             256\n","     LinearBlock-177            [-1, 128, 7, 7]               0\n","DepthWiseResidual-178            [-1, 128, 7, 7]               0\n","          Conv2d-179            [-1, 256, 7, 7]          32,768\n","     BatchNorm2d-180            [-1, 256, 7, 7]             512\n","           PReLU-181            [-1, 256, 7, 7]             256\n","       ConvBlock-182            [-1, 256, 7, 7]               0\n","          Conv2d-183            [-1, 256, 7, 7]           2,304\n","     BatchNorm2d-184            [-1, 256, 7, 7]             512\n","           PReLU-185            [-1, 256, 7, 7]             256\n","       ConvBlock-186            [-1, 256, 7, 7]               0\n","          Conv2d-187            [-1, 128, 7, 7]          32,768\n","     BatchNorm2d-188            [-1, 128, 7, 7]             256\n","     LinearBlock-189            [-1, 128, 7, 7]               0\n","DepthWiseResidual-190            [-1, 128, 7, 7]               0\n","        Residual-191            [-1, 128, 7, 7]               0\n","          Conv2d-192            [-1, 512, 7, 7]          65,536\n","     BatchNorm2d-193            [-1, 512, 7, 7]           1,024\n","           PReLU-194            [-1, 512, 7, 7]             512\n","       ConvBlock-195            [-1, 512, 7, 7]               0\n","          Conv2d-196            [-1, 512, 1, 1]          25,088\n","     BatchNorm2d-197            [-1, 512, 1, 1]           1,024\n","     LinearBlock-198            [-1, 512, 1, 1]               0\n","         Flatten-199                  [-1, 512]               0\n","          Linear-200                  [-1, 512]         262,144\n","     BatchNorm1d-201                  [-1, 512]           1,024\n","================================================================\n","Total params: 953,856\n","Trainable params: 953,856\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.14\n","Forward/backward pass size (MB): 95.68\n","Params size (MB): 3.64\n","Estimated Total Size (MB): 99.46\n","----------------------------------------------------------------\n","[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n","[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n","[INFO] Register count_prelu() for <class 'torch.nn.modules.activation.PReLU'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm1d'>.\n","/usr/local/lib/python3.7/dist-packages/thop/vision/calc_func.py:53: UserWarning: This API is being deprecated\n","  warnings.warn(\"This API is being deprecated\")\n","FLOPS=0.188225536G\n","PARAMS=0.953856M\n"]}],"source":["!python see_model.py"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":338,"status":"ok","timestamp":1655128198019,"user":{"displayName":"N26104214劉恩溢","userId":"03842490293544196844"},"user_tz":-480},"id":"TARJivZLMZhG","outputId":"bd86c9ad-b96a-4842-fd3b-3d3e97702324"},"outputs":[{"name":"stdout","output_type":"stream","text":["349 357\n"]},{"data":{"text/plain":["<function TextIOWrapper.close>"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["f = open(\"/content/gdrive/My Drive/Colab Notebooks/face/Pytorch-MobileFaceNet/dataset/lfw_test/lfw_test.txt\")\n","one=0\n","zero=0\n","for line in f.readlines():\n","  temp=line.split(\" \")\n","  if int(temp[2])==1:\n","    one=one+1\n","  else:\n","    zero=zero+1\n","print(one,zero)\n","f.close"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMAb0r80XeAQNzW1lCFcpNX","collapsed_sections":[],"name":"MobileFaceNet.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
